{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Laboratorio .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golivetti/Diplomatura-DL2020/blob/main/Laboratorio_TNG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MYaQwP0dLGf"
      },
      "source": [
        "# Naive Bayes Discreto\n",
        "\n",
        "Haremos un clasificador de artículos utilizando un modelo de Naive Bayes discreto. Trabajaremos con el dataset de Twenty News Group. Antes de empezar carguemos el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnSlIWEac1mm"
      },
      "source": [
        "#Loading the data set - training data.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-XZO1Gc2v_",
        "outputId": "48e7e8d0-ecf3-4ff2-c0d1-25f3b07a6690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "twenty_train.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvtYC0qndcGJ",
        "outputId": "f41c0a14-1877-469f-d164-10599517d96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(twenty_train.data) #Cantidad de artículos periodísticos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2MddId4d7EF",
        "outputId": "c6829598-f9ea-4a75-a174-d9b4b9a9816a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set(twenty_train[\"target\"]) #Clasificaciones de los artículos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJZkpO1SeAcq",
        "outputId": "6e418534-f83c-487c-9eca-13a1509919ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "twenty_train[\"target_names\"] #Referencia de los números de target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0byXL5-eFcx",
        "outputId": "71aa38f4-297b-4e1e-ba8c-b8af34d48be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "twenty_train.data[0] # Primer artículo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"From: cubbie@garnet.berkeley.edu (                               )\\nSubject: Re: Cubs behind Marlins? How?\\nArticle-I.D.: agate.1pt592$f9a\\nOrganization: University of California, Berkeley\\nLines: 12\\nNNTP-Posting-Host: garnet.berkeley.edu\\n\\n\\ngajarsky@pilot.njin.net writes:\\n\\nmorgan and guzman will have era's 1 run higher than last year, and\\n the cubs will be idiots and not pitch harkey as much as hibbard.\\n castillo won't be good (i think he's a stud pitcher)\\n\\n       This season so far, Morgan and Guzman helped to lead the Cubs\\n       at top in ERA, even better than THE rotation at Atlanta.\\n       Cubs ERA at 0.056 while Braves at 0.059. We know it is early\\n       in the season, we Cubs fans have learned how to enjoy the\\n       short triumph while it is still there.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alhF9Z41eQhA"
      },
      "source": [
        "Vamos a aplicar el siguiente procesamiento utilizando los conceptos vistos en clase:\n",
        "\n",
        "* Tokenization (nltk)\n",
        "* Lemmatization (nltk)\n",
        "* Stop Words (nltk)\n",
        "* Stemming (nltk)\n",
        "* Filtrado de palabras\n",
        "* Obtención del vocabulario (countvectorizer)\n",
        "* Transformación de los artículos en vectores\n",
        "* Armado del modelo de Naive Bayes Multinomial\n",
        "* Evaluación con el Train Set\n",
        "* Evaluación con el Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVNxGZHMeJsH",
        "outputId": "96c6e1b1-fe8a-4580-c1c3-ce3bc2d41d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtB9WY00ewxM",
        "outputId": "248e576e-b8aa-4b6b-8a87-89589f6fef0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#Procesando todos los artículos:\n",
        "articulos_procesados=list()\n",
        "for idx in range(len(twenty_train.data)):\n",
        "    if idx%1000==0:\n",
        "        print(f'se procesaron {idx} artículos')\n",
        "    art=twenty_train.data[idx]\n",
        "    tok=word_tokenize(art)\n",
        "    lem=[lemmatizer.lemmatize(x,pos='v') for x in tok]\n",
        "    stop = [x for x in lem if x not in stopwords.words('english')]\n",
        "    stem=[stemmer.stem(x) for x in stop]\n",
        "    alpha=[x for x in stem if x.isalpha()]\n",
        "    articulos_procesados.append(\" \".join(alpha))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "se procesaron 0 artículos\n",
            "se procesaron 1000 artículos\n",
            "se procesaron 2000 artículos\n",
            "se procesaron 3000 artículos\n",
            "se procesaron 4000 artículos\n",
            "se procesaron 5000 artículos\n",
            "se procesaron 6000 artículos\n",
            "se procesaron 7000 artículos\n",
            "se procesaron 8000 artículos\n",
            "se procesaron 9000 artículos\n",
            "se procesaron 10000 artículos\n",
            "se procesaron 11000 artículos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKFI92vSkpv3",
        "outputId": "f0597a54-9b1a-4a04-deef-966cd99a95a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Extracting features from articles\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer(max_df=0.6,min_df=10) #chequear que max_df y min_df sean los que se piden\n",
        "count_vect.fit(articulos_procesados) #Aprende el vocabulario y le asigna un código a cada palabra"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=0.6, max_features=None, min_df=10,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuKEHS8nJgz"
      },
      "source": [
        "En la siguiente celda de código transforme los artículos procesados al vector de cuentas de palabras, es decir, transforme los artículos procesados utilizando el count vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0aDvu7_niWR"
      },
      "source": [
        "X_train_data=count_vect.transform(articulos_procesados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvp0yqMvn5u8"
      },
      "source": [
        "Utilice la función MultinomialNB de sklear para implementar un clasificador Naive Bayes discreto. Utilice smoothing laplaciano con alpha=3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKbjTWPPlYU3",
        "outputId": "8bdc817a-3fed-4a98-ad3e-a74e02f90f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB(alpha=3)\n",
        "clf.fit(X_train_data.toarray(),twenty_train['target'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=3, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtG52ZJdpMOV"
      },
      "source": [
        "## Evaluación con el train set\n",
        "Evalúe el accuracy del modelo entrenado utilizando el train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXk0xVJrnxk-",
        "outputId": "9621c221-3c74-4e71-a89f-3579fada908f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf.score(X_train_data.toarray(), twenty_train['target'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9129397207000177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYCjQykXq0of"
      },
      "source": [
        "# Evaluación con el test set\n",
        "Procese y convierta los artículos del test-set. Evalúe el accuracy del modelo con los parámetros obtenidos anteriormente utilizando el test-set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_T_6JWGoedN"
      },
      "source": [
        "twenty_test = fetch_20newsgroups(subset='test', shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoX-jgDdpJlf",
        "outputId": "8b50930d-c036-4040-8797-cf379f71db75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Procesando todos los artículos:\n",
        "articulos_procesados_test=list()\n",
        "for idx in range(len(twenty_test.data)):\n",
        "    if idx%1000==0:\n",
        "        print(f'se procesaron {idx} artículos')\n",
        "    art=twenty_test.data[idx]\n",
        "    tok=word_tokenize(art)\n",
        "    lem=[lemmatizer.lemmatize(x,pos='v') for x in tok]\n",
        "    stop = [x for x in lem if x not in stopwords.words('english')]\n",
        "    stem=[stemmer.stem(x) for x in stop]\n",
        "    alpha=[x for x in stem if x.isalpha()]\n",
        "    articulos_procesados_test.append(\" \".join(alpha))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "se procesaron 0 artículos\n",
            "se procesaron 1000 artículos\n",
            "se procesaron 2000 artículos\n",
            "se procesaron 3000 artículos\n",
            "se procesaron 4000 artículos\n",
            "se procesaron 5000 artículos\n",
            "se procesaron 6000 artículos\n",
            "se procesaron 7000 artículos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJlssjrrlA7"
      },
      "source": [
        "#Transforme los artículos de test procesados\n",
        "X_test_data=count_vect.transform(articulos_procesados_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abq_GeMFpw8c",
        "outputId": "a0a521f8-8322-4010-eb6a-9a8426f349fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Evalúe el score del modelo entrenado para el train set para los artículos de test\n",
        "clf.score(X_test_data.toarray(), twenty_test['target'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7841210833775889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZJjJNg9d-52",
        "outputId": "d447f71b-2e04-412a-87db-febc7df6adcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7532, 9139)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUy8RXO3eChI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}